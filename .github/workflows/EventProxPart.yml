name: EventProxPart (FlowAgility eventos + participantes) Beta

on:
  workflow_dispatch:
    inputs:
      limit_events:
        description: "M√°ximo de eventos a procesar (0 = sin l√≠mite)"
        required: false
        default: "2"
      max_runtime_min:
        description: "Corte ordenado a los N minutos (0 = sin l√≠mite)"
        required: false
        default: "15"
  schedule:
    # 03:40 UTC ‚âà 05:40 Europe/Madrid (seg√∫n DST)
    - cron: "40 3 * * *"

permissions:
  contents: read

concurrency:
  group: eventproxpart-${{ github.ref }}
  cancel-in-progress: false

jobs:
  run-scrape:
    runs-on: ubuntu-24.04
    timeout-minutes: 45

    env:
      TZ: Europe/Madrid
      PYTHONUNBUFFERED: "1"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"

      # ---- Scraper ENV ----
      HEADLESS: "true"
      INCOGNITO: "true"
      OUT_DIR: "./output"
      MAX_SCROLLS: "8"
      SCROLL_WAIT_S: "2.0"
      
      # üîß MODO PRUEBAS - CONFIGURACI√ìN OPTIMIZADA
      MAX_EVENTS_FOR_TESTING: "${{ github.event.inputs.limit_events || '2' }}"
      MAX_RUNTIME_MIN: "${{ github.event.inputs.max_runtime_min || '15' }}"

      # Throttling optimizado para pruebas
      THROTTLE_EVENT_S: "2.0"
      THROTTLE_PAGE_MIN_S: "1.0"
      THROTTLE_PAGE_MAX_S: "2.0"
      THROTTLE_TOGGLE_MIN_S: "0.8"
      THROTTLE_TOGGLE_MAX_S: "1.5"

      # Credenciales (usar Secrets)
      FLOW_EMAIL: "${{ secrets.FLOW_EMAIL }}"
      FLOW_PASS: "${{ secrets.FLOW_PASS }}"

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y wget curl unzip

      - name: Install Chrome Browser
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          echo "Chrome version:"
          google-chrome-stable --version

      - name: Install ChromeDriver
        run: |
          # Instalar chromedriver compatible
          CHROME_VERSION=$(google-chrome-stable --version | grep -oP '\d+\.\d+\.\d+\.\d+' | cut -d. -f1-3)
          echo "Chrome version: $CHROME_VERSION"
          
          # Descargar chromedriver correspondiente
          CHROMEDRIVER_VERSION=$(curl -s "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_$CHROME_VERSION")
          wget -q "https://chromedriver.storage.googleapis.com/$CHROMEDRIVER_VERSION/chromedriver_linux64.zip"
          unzip -q chromedriver_linux64.zip
          sudo mv chromedriver /usr/local/bin/
          sudo chmod +x /usr/local/bin/chromedriver
          rm chromedriver_linux64.zip
          
          echo "ChromeDriver version:"
          chromedriver --version

      - name: Verify Chrome and ChromeDriver
        run: |
          echo "=== VERIFICACI√ìN DE INSTALACI√ìN ==="
          which google-chrome-stable && google-chrome-stable --version || echo "‚ùå Chrome no encontrado"
          which chromedriver && chromedriver --version || echo "‚ùå ChromeDriver no encontrado"
          
          # Verificar que Chrome puede ejecutarse
          timeout 10s google-chrome-stable --headless --no-sandbox --disable-gpu --dump-dom https://example.com > /dev/null 2>&1 && \
          echo "‚úÖ Chrome funciona correctamente" || \
          echo "‚ö†Ô∏è  Chrome tiene problemas de ejecuci√≥n"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4 python-dotenv
          pip list | grep -E "(selenium|beautifulsoup|dotenv)"

      - name: Create output directory
        run: |
          mkdir -p "${{ env.OUT_DIR }}"
          echo "Directorio de salida creado: ${{ env.OUT_DIR }}"
          ls -la

      - name: Create .env file from secrets
        run: |
          echo "FLOW_EMAIL=${{ secrets.FLOW_EMAIL }}" > .env
          echo "FLOW_PASS=${{ secrets.FLOW_PASS }}" >> .env
          echo "HEADLESS=${{ env.HEADLESS }}" >> .env
          echo "INCOGNITO=${{ env.INCOGNITO }}" >> .env
          echo "OUT_DIR=${{ env.OUT_DIR }}" >> .env
          echo "MAX_SCROLLS=${{ env.MAX_SCROLLS }}" >> .env
          echo "SCROLL_WAIT_S=${{ env.SCROLL_WAIT_S }}" >> .env
          
          # Mostrar .env (sin password)
          echo "=== ARCHIVO .env CONFIGURADO ==="
          cat .env | sed 's/FLOW_PASS=.*/FLOW_PASS=***/'

      - name: Run EventosProxParticipantesDeep.py scraper
        timeout-minutes: 30
        env:
          FLOW_EMAIL: "${{ secrets.FLOW_EMAIL }}"
          FLOW_PASS: "${{ secrets.FLOW_PASS }}"
        run: |
          echo "=== INICIANDO SCRAPER ==="
          echo "Script: EventosProxParticipantesDeep.py"
          echo "M√≥dulo: all"
          echo "L√≠mite de eventos: ${{ env.MAX_EVENTS_FOR_TESTING }}"
          echo "Timeout m√°ximo: ${{ env.MAX_RUNTIME_MIN }} minutos"
          echo "Directorio salida: ${{ env.OUT_DIR }}"
          echo ""
          
          # Ejecutar el scraper
          python EventosProxParticipantesDeep.py --module all
          
          SCRAPER_EXIT_CODE=$?
          echo "Scraper finalizado con c√≥digo: $SCRAPER_EXIT_CODE"
          
          if [ $SCRAPER_EXIT_CODE -eq 0 ]; then
            echo "‚úÖ Scraper ejecutado exitosamente"
          else
            echo "‚ùå Scraper fall√≥ con c√≥digo $SCRAPER_EXIT_CODE"
            # No salir inmediatamente para permitir debugging
          fi

      - name: Verify generated JSON files
        run: |
          echo "=== VERIFICACI√ìN DE ARCHIVOS GENERADOS ==="
          echo "Contenido del directorio ${{ env.OUT_DIR }}:"
          ls -la "${{ env.OUT_DIR }}" || echo "‚ùå No se pudo listar el directorio"
          
          # Verificar archivos esperados
          REQUIRED_FILES=("01events.json" "02info.json")
          ALL_FILES_EXIST=true
          
          for file in "${REQUIRED_FILES[@]}"; do
            FILE_PATH="${{ env.OUT_DIR }}/$file"
            if [ -f "$FILE_PATH" ]; then
              FILE_SIZE=$(stat -c%s "$FILE_PATH")
              FILE_LINES=$(wc -l < "$FILE_PATH" 2>/dev/null || echo "N/A")
              echo "‚úÖ $file - Tama√±o: ${FILE_SIZE} bytes, L√≠neas: ${FILE_LINES}"
              
              # Mostrar preview del contenido
              if [ "$FILE_SIZE" -gt 0 ]; then
                echo "   Primeras 3 l√≠neas:"
                head -3 "$FILE_PATH" | sed 's/^/      /'
              fi
            else
              echo "‚ùå $file - NO ENCONTRADO"
              ALL_FILES_EXIST=false
            fi
            echo ""
          done
          
          # Verificar tambi√©n archivos con fecha
          DATED_FILES=$(ls "${{ env.OUT_DIR }}"/0*.json 2>/dev/null | wc -l || echo "0")
          echo "üìÖ Archivos con fecha encontrados: $DATED_FILES"
          
          if [ "$ALL_FILES_EXIST" = true ]; then
            echo "‚úÖ TODOS los archivos requeridos est√°n presentes"
          else
            echo "‚ùå Faltan algunos archivos requeridos"
            # No fallar el workflow para permitir debugging
          fi

      - name: Analyze JSON content
        run: |
          echo "=== AN√ÅLISIS DE CONTENIDO JSON ==="
          
          # Analizar 01events.json
          if [ -f "${{ env.OUT_DIR }}/01events.json" ]; then
            echo "üìä An√°lisis de 01events.json:"
            python -c "
import json, os
try:
    with open('${{ env.OUT_DIR }}/01events.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f'   N√∫mero de eventos: {len(data)}')
    for i, event in enumerate(data[:3]):  # Mostrar primeros 3
        print(f'   Evento {i+1}: {event.get(\"nombre\", \"N/A\")}')
        print(f'     ID: {event.get(\"id\", \"N/A\")}')
        print(f'     Club: {event.get(\"club\", \"N/A\")}')
        print(f'     Enlace participantes: {event.get(\"enlaces\", {}).get(\"participantes\", \"N/A\")}')
except Exception as e:
    print(f'   Error analizando 01events.json: {e}')
"
          fi
          
          # Analizar 02info.json
          if [ -f "${{ env.OUT_DIR }}/02info.json" ]; then
            echo ""
            echo "üìä An√°lisis de 02info.json:"
            python -c "
import json, os
try:
    with open('${{ env.OUT_DIR }}/02info.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f'   N√∫mero de eventos procesados: {len(data)}')
    
    events_with_participants = [e for e in data if e.get('numero_participantes', 0) > 0]
    total_participants = sum(e.get('numero_participantes', 0) for e in data)
    
    print(f'   Eventos con participantes: {len(events_with_participants)}')
    print(f'   Total participantes: {total_participants}')
    
    if events_with_participants:
        print('   Eventos con m√°s participantes:')
        sorted_events = sorted(events_with_participants, key=lambda x: x.get('numero_participantes', 0), reverse=True)
        for event in sorted_events[:3]:
            print(f'     - {event.get(\"nombre\", \"N/A\")}: {event.get(\"numero_participantes\")} participantes')
except Exception as e:
    print(f'   Error analizando 02info.json: {e}')
"
          fi

      - name: Compress JSON files for backup
        run: |
          echo "=== COMPRIMIENDO ARCHIVOS PARA BACKUP ==="
          
          # Comprimir archivos JSON
          for json_file in "${{ env.OUT_DIR }}"/*.json; do
            if [ -f "$json_file" ]; then
              gzip -9 -k "$json_file"
              original_size=$(stat -c%s "$json_file")
              compressed_size=$(stat -c%s "${json_file}.gz")
              compression_ratio=$((compressed_size * 100 / original_size))
              echo "‚úÖ $(basename "$json_file"): ${original_size} bytes ‚Üí ${compressed_size} bytes (${compression_ratio}%)"
            fi
          done
          
          echo ""
          echo "Archivos comprimidos:"
          ls -la "${{ env.OUT_DIR }}"/*.gz 2>/dev/null || echo "No hay archivos comprimidos"

      - name: Create backup directory
        run: |
          mkdir -p ./backup
          echo "Directorio backup creado"

      - name: Backup uncompressed files
        run: |
          echo "=== CREANDO BACKUP DE ARCHIVOS SIN COMPRIMIR ==="
          cp -r "${{ env.OUT_DIR }}" ./backup/ || echo "No se pudo crear backup"
          echo "Backup creado en ./backup/output/"
          ls -la ./backup/ || echo "No hay backup"

      - name: Upload JSON files as artifact
        uses: actions/upload-artifact@v4
        with:
          name: scraper-json-output
          path: |
            ${{ env.OUT_DIR }}/*.json
            ${{ env.OUT_DIR }}/*.gz
          retention-days: 7
          if-no-files-found: warn

      - name: Upload backup as artifact
        uses: actions/upload-artifact@v4
        with:
          name: scraper-backup
          path: |
            ./backup/
          retention-days: 3
          if-no-files-found: warn

      - name: Upload logs and debugging info
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: |
            ./*.log
            /tmp/*.png
            /home/runner/.local/share/chromium/
          retention-days: 1
          if-no-files-found: ignore

      - name: Final summary
        run: |
          echo "=== RESUMEN FINAL DEL WORKFLOW ==="
          echo "üéØ Configuraci√≥n utilizada:"
          echo "   - L√≠mite de eventos: ${{ env.MAX_EVENTS_FOR_TESTING }}"
          echo "   - M√°ximo tiempo ejecuci√≥n: ${{ env.MAX_RUNTIME_MIN }} minutos"
          echo "   - Directorio salida: ${{ env.OUT_DIR }}"
          
          echo ""
          echo "üìÅ Archivos generados:"
          if [ -d "${{ env.OUT_DIR }}" ]; then
            for file in "${{ env.OUT_DIR }}"/*; do
              if [ -f "$file" ]; then
                size=$(stat -c%s "$file" 2>/dev/null || echo "N/A")
                echo "   - $(basename "$file"): $size bytes"
              fi
            done
          else
            echo "   ‚ùå No se generaron archivos"
          fi
          
          echo ""
          echo "‚è∞ Hora de finalizaci√≥n: $(date)"
          echo "‚úÖ Workflow completado"

      - name: Debug on failure
        if: failure()
        run: |
          echo "=== DEBUGGING POR FALLO ==="
          echo "‚ùå El workflow ha fallado"
          
          echo ""
          echo "üìã Informaci√≥n del sistema:"
          echo "Ubuntu version: $(lsb_release -d || echo 'N/A')"
          echo "Python version: $(python --version || echo 'N/A')"
          echo "Chrome version: $(google-chrome-stable --version || echo 'N/A')"
          echo "ChromeDriver version: $(chromedriver --version || echo 'N/A')"
          
          echo ""
          echo "üìÅ Estructura de directorios:"
          echo "Directorio actual:"
          ls -la
          echo ""
          echo "Directorio output:"
          ls -la "${{ env.OUT_DIR }}" 2>/dev/null || echo "No existe"
          echo ""
          echo "Directorio backup:"
          ls -la "./backup" 2>/dev/null || echo "No existe"
          
          echo ""
          echo "üîç Procesos Chrome activos:"
          ps aux | grep -i chrome || echo "No hay procesos Chrome"
          
          echo ""
          echo "üìä √öltimos logs del sistema:"
          dmesg | tail -20 || echo "No se pueden leer logs del sistema"

  # Job opcional para despliegue (solo en ejecuciones manuales)
  deploy-to-ftp:
    if: github.event_name == 'workflow_dispatch'
    needs: run-scrape
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          name: scraper-json-output
          path: ./artifacts
          
      - name: Check downloaded files
        run: |
          echo "=== ARCHIVOS DESCARGADOS ==="
          find ./artifacts -type f -name "*.json" -o -name "*.gz" | sort
          echo ""
          echo "Contenido:"
          ls -la ./artifacts/
          
      - name: Upload to FTP (conditional)
        if: env.FTP_SERVER != '' && env.FTP_USERNAME != '' && env.FTP_PASSWORD != ''
        env:
          FTP_SERVER: ${{ secrets.FTP_SERVER }}
          FTP_USERNAME: ${{ secrets.FTP_USERNAME }}
          FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}
          FTP_REMOTE_DIR: ${{ secrets.FTP_REMOTE_DIR }}
        run: |
          echo "=== SUBIENDO A FTP (MODO PRUEBAS) ==="
          
          if [ -z "$FTP_SERVER" ] || [ -z "$FTP_USERNAME" ] || [ -z "$FTP_PASSWORD" ]; then
            echo "‚ö†Ô∏è  Credenciales FTP no configuradas - saltando subida"
            exit 0
          fi
          
          REMOTE_DIR="${FTP_REMOTE_DIR}/Competiciones/EventosProx/Flow/data/TEST"
          BASE_URL="ftp://${FTP_SERVER}${REMOTE_DIR}"
          
          echo "Subiendo a: ${BASE_URL}/"
          
          # Crear directorio remoto
          curl --fail --silent \
               --ssl-reqd \
               --user "${FTP_USERNAME}:${FTP_PASSWORD}" \
               -Q "MKD ${REMOTE_DIR}" \
               "ftp://${FTP_SERVER}/" || echo "Directorio posiblemente ya existe"
          
          # Subir archivos JSON
          for json_file in ./artifacts/*.json; do
            if [ -f "$json_file" ]; then
              filename=$(basename "$json_file")
              echo "üì§ Subiendo: $filename"
              
              curl --fail --silent \
                   --ssl-reqd \
                   --user "${FTP_USERNAME}:${FTP_PASSWORD}" \
                   --upload-file "$json_file" \
                   "${BASE_URL}/$filename" && \
              echo "‚úÖ $filename subido exitosamente" || \
              echo "‚ùå Error subiendo $filename"
            fi
          done
          
          echo "‚úÖ Subida FTP completada"

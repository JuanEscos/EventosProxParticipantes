name: EventProxPart (FlowAgility eventos + participantes)

on:
  workflow_dispatch:
    inputs:
      limit_events:
        description: "M√°ximo de eventos a procesar (0 = sin l√≠mite)"
        required: false
        default: "35"
      max_runtime_min:
        description: "Cortar ordenadamente a los N minutos (0 = sin l√≠mite)"
        required: false
        default: "25"
  schedule:
    # 03:40 UTC ‚âà 05:40 Europe/Madrid (seg√∫n DST)
    - cron: "40 3 * * *"

permissions:
  contents: read

concurrency:
  group: eventproxpart-${{ github.ref }}
  cancel-in-progress: false

jobs:
  run-scrape:
    runs-on: ubuntu-24.04
    timeout-minutes: 90

    env:
      TZ: Europe/Madrid
      PYTHONUNBUFFERED: "1"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"

      # ---- Scraper ENV ----
      HEADLESS: "true"
      INCOGNITO: "true"
      OUT_DIR: "./output"
      MAX_SCROLLS: "12"
      SCROLL_WAIT_S: "2.0"
      LIMIT_EVENTS: "${{ github.event.inputs.limit_events || '35' }}"
      MAX_RUNTIME_MIN: "${{ github.event.inputs.max_runtime_min || '25' }}"

      # Throttling + resume
      THROTTLE_EVENT_S: "3.0"
      THROTTLE_PAGE_MIN_S: "1.2"
      THROTTLE_PAGE_MAX_S: "2.5"
      THROTTLE_TOGGLE_MIN_S: "0.9"
      THROTTLE_TOGGLE_MAX_S: "2.2"
      AUTO_SAVE_EVERY: "10"
      RESUME: "true"
      RESUME_FILE: ""

      # Credenciales Flow (secrets)
      FLOW_EMAILRq: "${{ secrets.FLOW_EMAILRQ }}"
      FLOW_PASSRq: "${{ secrets.FLOW_PASSRQ }}"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install Chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable

      - name: Install chromedriver
        uses: nanasess/setup-chromedriver@v2

      - name: Show Chrome versions
        run: |
          google-chrome --version || true
          google-chrome-stable --version || true
          chromedriver --version || true

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else:
            pip install selenium webdriver-manager beautifulsoup4 python-dotenv
          fi

      - name: Prepare output dir
        run: |
          mkdir -p "${OUT_DIR}"
          echo "OUT_DIR contents (before):"
          ls -la "${OUT_DIR}" || true
          
      - name: Syntax check
        run: |
          python -m py_compile ./02EventosProxParticipantesGitHubGPT.py
    
      - name: Run Python scraper (events + participants)
        timeout-minutes: 60
        env:
          HEADLESS: "${{ env.HEADLESS }}"
          INCOGNITO: "${{ env.INCOGNITO }}"
          OUT_DIR: "${{ env.OUT_DIR }}"
          MAX_SCROLLS: "${{ env.MAX_SCROLLS }}"
          SCROLL_WAIT_S: "${{ env.SCROLL_WAIT_S }}"
          LIMIT_EVENTS: "${{ env.LIMIT_EVENTS }}"
          MAX_RUNTIME_MIN: "${{ env.MAX_RUNTIME_MIN }}"
          THROTTLE_EVENT_S: "${{ env.THROTTLE_EVENT_S }}"
          THROTTLE_PAGE_MIN_S: "${{ env.THROTTLE_PAGE_MIN_S }}"
          THROTTLE_PAGE_MAX_S: "${{ env.THROTTLE_PAGE_MAX_S }}"
          THROTTLE_TOGGLE_MIN_S: "${{ env.THROTTLE_TOGGLE_MIN_S }}"
          THROTTLE_TOGGLE_MAX_S: "${{ env.THROTTLE_TOGGLE_MAX_S }}"
          AUTO_SAVE_EVERY: "${{ env.AUTO_SAVE_EVERY }}"
          RESUME: "${{ env.RESUME }}"
          RESUME_FILE: "${{ env.RESUME_FILE }}"
          FLOW_EMAILRq: "${{ env.FLOW_EMAILRq }}"
          FLOW_PASSRq: "${{ env.FLOW_PASSRq }}"
        run: |
          echo "=== EJECUTANDO SCRAPER ==="
          # üëâ fuerza l√≠mite de eventos tambi√©n por CLI
          python ./02EventosProxParticipantesGitHubGPT.py --module all --limit-events "${LIMIT_EVENTS}"
          echo "=== SCRAPER COMPLETADO ==="

      - name: Quick quality report (JSON completeness)
        if: always()
        shell: bash
        run: |
          echo "=== CALIDAD DE CAMPOS (participantes_detallados.json) ==="
          if [ -f "./output/participantes_detallados.json" ]; then
            python - <<'PY'
import json, sys

pth = "./output/participantes_detallados.json"
try:
    with open(pth, "r", encoding="utf-8") as f:
        data = json.load(f)
except Exception as e:
    print(f"No se pudo leer {pth}: {e}")
    sys.exit(0)

rows = 0
counts = {}
for ev in (data or []):
    for p in ev.get("participantes", []):
        rows += 1
        for k, v in p.items():
            d = counts.setdefault(k, {"empty": 0, "total": 0})
            d["total"] += 1
            if v is None or str(v).strip() == "":
                d["empty"] += 1

print(f"Participantes totales: {rows}")
for k in sorted(counts):
    t = counts[k]["total"]; e = counts[k]["empty"]
    pct = (e * 100.0 / t) if t else 0.0
    print(f"{k:22s} vac√≠os: {e:4d}/{t:4d}  ({pct:5.1f}%)")
PY
          else
            echo "No existe ./output/participantes_detallados.json"
          fi


      - name: Verify generated files
        run: |
          echo '=== VERIFICANDO ARCHIVOS GENERADOS ==='
          ls -la ./output/ || true

          MUST_HAVE=("01events.json")
          ONE_OF=("02info.json" "participantes_detallados.json")

          missing=0
          for file in "${MUST_HAVE[@]}"; do
            if [ -f "./output/$file" ]; then
              sz=$(stat -c%s "./output/$file")
              echo "‚úÖ $file: ENCONTRADO (${sz} bytes)"
            else
              echo "‚ùå $file: NO ENCONTRADO"
              missing=$((missing+1))
            fi
          done

          found_optional=0
          for opt in "${ONE_OF[@]}"; do
            if [ -f "./output/$opt" ]; then
              sz=$(stat -c%s "./output/$opt")
              echo "‚úÖ $opt: ENCONTRADO (${sz} bytes)"
              found_optional=1
            else
              echo "‚ö†Ô∏è  $opt: no encontrado"
            fi
          done

          if [ $missing -eq 0 ] && [ $found_optional -eq 1 ]; then
            echo "‚úÖ Archivos m√≠nimos presentes"
          else
            echo "‚ùå Faltan archivos requeridos (01events.json) o ninguno de (02info.json|participantes_detallados.json)"
            exit 1
          fi

      - name: Compress JSON files for FTP
        run: |
          echo "=== COMPRIMIENDO ARCHIVOS PARA FTP ==="
          to_compress=( "01events.json" "02info.json" "participantes_detallados.json" )
          for f in "${to_compress[@]}"; do
            if [ -f "./output/$f" ]; then
              echo "Comprimiendo: $f"
              gzip -9 -c "./output/$f" > "./output/${f}.gz"
              orig=$(stat -c%s "./output/$f")
              comp=$(stat -c%s "./output/${f}.gz")
              ratio=$(( comp * 100 / orig ))
              echo "  $orig bytes ‚Üí $comp bytes (${ratio}%)"
            fi
          done
          echo "=== ARCHIVOS COMPRIMIDOS ==="
          ls -la ./output/*.gz || true

      - name: Upload compressed files to FTP
        env:
          FTP_SERVER: ${{ secrets.FTP_SERVER }}
          FTP_USERNAME: ${{ secrets.FTP_USERNAME }}
          FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}
          FTP_REMOTE_DIR: ${{ secrets.FTP_REMOTE_DIR }}
        run: |
          echo '=== SUBIENDO ARCHIVOS COMPRIMIDOS ==='

          if [ -z "$FTP_SERVER" ] || [ -z "$FTP_USERNAME" ] || [ -z "$FTP_PASSWORD" ]; then
            echo "‚ùå ERROR: Variables FTP no configuradas correctamente"
            exit 1
          fi

          REMOTE_DIR="${FTP_REMOTE_DIR}/Competiciones/EventosProx/Flow/data"
          BASE_URL="ftp://${FTP_SERVER}${REMOTE_DIR}"
          echo "Subiendo a: ${BASE_URL}/"

          COMPRESSED_FILES=()
          for f in "01events.json.gz" "02info.json.gz" "participantes_detallados.json.gz"; do
            [ -f "./output/$f" ] && COMPRESSED_FILES+=("$f")
          done

          if [ ${#COMPRESSED_FILES[@]} -eq 0 ]; then
            echo "‚ùå No hay archivos comprimidos para subir"
            exit 1
          fi

          for file in "${COMPRESSED_FILES[@]}"; do
            local_file="./output/$file"
            echo "üì§ Subiendo: $file"
            curl --fail \
                 --ssl-reqd \
                 --ftp-create-dirs \
                 --disable-epsv \
                 --ftp-skip-pasv-ip \
                 --user "${FTP_USERNAME}:${FTP_PASSWORD}" \
                 --upload-file "$local_file" \
                 "${BASE_URL}/$file" \
            && echo "‚úÖ $file subido" || {
              echo "‚ùå Error subiendo $file"
              orig="${file%.gz}"
              if [ -f "./output/$orig" ]; then
                echo "üîÑ Intentando subir original: $orig"
                curl --fail \
                     --ssl-reqd \
                     --user "${FTP_USERNAME}:${FTP_PASSWORD}" \
                     --upload-file "./output/$orig" \
                     "${BASE_URL}/$orig" \
                && echo "‚úÖ $orig subido" || echo "‚ùå Error subiendo $orig tambi√©n"
              fi
            }
            sleep 1
          done

          if [ -f "./output/01events.json" ]; then
            echo "üì§ Subiendo 01events.json (sin comprimir)"
            curl --fail \
                 --ssl-reqd \
                 --user "${FTP_USERNAME}:${FTP_PASSWORD}" \
                 --upload-file "./output/01events.json" \
                 "${BASE_URL}/01events.json" \
            && echo "‚úÖ 01events.json subido" || echo "‚ö†Ô∏è  No se pudo subir 01events.json"
          fi

      - name: Create backup directory
        run: mkdir -p ./backup

      - name: Backup uncompressed files
        run: |
          echo "=== CREANDO BACKUP LOCAL ==="
          [ -f ./output/01events.json ] && cp ./output/01events.json ./backup/ || true
          [ -f ./output/02info.json ] && cp ./output/02info.json ./backup/ || true
          [ -f ./output/participantes_detallados.json ] && cp ./output/participantes_detallados.json ./backup/ || true
          echo "Backup creado en ./backup/"
          ls -la ./backup/ || true

      - name: Upload backup as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: json-backup-uncompressed
          path: ./backup/*.json
          if-no-files-found: warn
          retention-days: 7

      - name: Upload compressed files as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: json-compressed
          path: ./output/*.gz
          if-no-files-found: warn
          retention-days: 3

      - name: Upload participant debug HTML (if any)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: participants-debug-html
          path: ./output/debug_part_*.html
          if-no-files-found: ignore
          retention-days: 5

      - name: Debug on failure
        if: failure()
        run: |
          echo "=== DEBUGGING FAILURE ==="
          echo "output/:"
          ls -la ./output/ 2>/dev/null || echo "No output directory"
          echo "backup/:"
          ls -la ./backup/ 2>/dev/null || echo "No backup directory"
          echo "=== CHROME ==="
          which google-chrome || true
          which google-chrome-stable || true
          which chromedriver || true
          echo "=== FILE SIZES ==="
          for f in ./output/*.json ./output/*.gz; do
            [ -f "$f" ] && echo "$(basename "$f"): $(stat -c%s "$f") bytes"
          done || true

name: EventProxPart (FlowAgility eventos + participantes) Beta

on:
  workflow_dispatch:
    inputs:
      limit_events:
        description: "M√°ximo de eventos a procesar (0 = sin l√≠mite)"
        required: false
        default: "2"
      max_runtime_min:
        description: "Corte ordenado a los N minutos (0 = sin l√≠mite)"
        required: false
        default: "15"
  schedule:
    # 03:40 UTC ‚âà 05:40 Europe/Madrid (seg√∫n DST)
    - cron: "40 3 * * *"

permissions:
  contents: read

concurrency:
  group: eventproxpart-${{ github.ref }}
  cancel-in-progress: false

jobs:
  run-scrape:
    runs-on: ubuntu-24.04
    timeout-minutes: 45

    env:
      TZ: Europe/Madrid
      PYTHONUNBUFFERED: "1"
      PIP_DISABLE_PIP_VERSION_CHECK: "1"

      # ---- Scraper ENV ----
      HEADLESS: "true"
      INCOGNITO: "true"
      OUT_DIR: "./output"
      MAX_SCROLLS: "8"
      SCROLL_WAIT_S: "2.0"
      
      # üîß MODO PRUEBAS - CONFIGURACI√ìN OPTIMIZADA
      MAX_EVENTS_FOR_TESTING: "${{ github.event.inputs.limit_events || '2' }}"
      MAX_RUNTIME_MIN: "${{ github.event.inputs.max_runtime_min || '60' }}"

      # Throttling optimizado para pruebas
      THROTTLE_EVENT_S: "2.0"
      THROTTLE_PAGE_MIN_S: "1.0"
      THROTTLE_PAGE_MAX_S: "2.0"
      THROTTLE_TOGGLE_MIN_S: "0.8"
      THROTTLE_TOGGLE_MAX_S: "1.5"

      # Credenciales (usar Secrets)
      FLOW_EMAIL: "${{ secrets.FLOW_EMAILRQ }}"
      FLOW_PASS: "${{ secrets.FLOW_PASSRQ }}"

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y wget curl unzip gnupg ca-certificates

      - name: Install Google Chrome
        run: |
          # A√±adir repo oficial de Google Chrome
          wget -qO - https://dl.google.com/linux/linux_signing_key.pub | sudo gpg --dearmor -o /usr/share/keyrings/google-linux.gpg
          echo "deb [arch=amd64 signed-by=/usr/share/keyrings/google-linux.gpg] https://dl.google.com/linux/chrome/deb/ stable main" | \
            sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          google-chrome --version

      - name: Install matching ChromeDriver
        run: |
          # Detectar versi√≥n mayor.minor.build de Chrome
          CHROME_VERSION=$(google-chrome --version | grep -oP '\d+\.\d+\.\d+\.\d+' | cut -d. -f1-3)
          echo "Chrome base version: $CHROME_VERSION"

          # Obtener la versi√≥n de ChromeDriver compatible
          CHROMEDRIVER_VERSION=$(curl -s "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_${CHROME_VERSION}")
          echo "Matching ChromeDriver: $CHROMEDRIVER_VERSION"

          wget -q "https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip"
          unzip -q chromedriver_linux64.zip
          sudo mv chromedriver /usr/local/bin/
          sudo chmod +x /usr/local/bin/chromedriver
          rm chromedriver_linux64.zip
          chromedriver --version

      - name: Verify Chrome and ChromeDriver
        run: |
          echo "=== VERIFICACI√ìN DE INSTALACI√ìN ==="
          which google-chrome && google-chrome --version || echo "‚ùå Chrome no encontrado"
          which chromedriver && chromedriver --version || echo "‚ùå ChromeDriver no encontrado"

          # Verificar que Chrome puede ejecutarse en headless
          timeout 10s google-chrome --headless --no-sandbox --disable-gpu --dump-dom https://example.com > /dev/null 2>&1 && \
          echo "‚úÖ Chrome funciona correctamente" || \
          echo "‚ö†Ô∏è  Chrome tiene problemas de ejecuci√≥n"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4 python-dotenv
          pip list | grep -E "(selenium|beautifulsoup|dotenv)"

      - name: Create output directory
        run: |
          mkdir -p "${{ env.OUT_DIR }}"
          echo "Directorio de salida creado: ${{ env.OUT_DIR }}"
          ls -la

      - name: Create .env file from env vars
        run: |
          echo "FLOW_EMAIL=${{ env.FLOW_EMAIL }}" > .env
          echo "FLOW_PASS=${{ env.FLOW_PASS }}" >> .env
          echo "HEADLESS=${{ env.HEADLESS }}" >> .env
          echo "INCOGNITO=${{ env.INCOGNITO }}" >> .env
          echo "OUT_DIR=${{ env.OUT_DIR }}" >> .env
          echo "MAX_SCROLLS=${{ env.MAX_SCROLLS }}" >> .env
          echo "SCROLL_WAIT_S=${{ env.SCROLL_WAIT_S }}" >> .env

          echo "=== ARCHIVO .env CONFIGURADO ==="
          cat .env | sed 's/FLOW_PASS=.*/FLOW_PASS=***/'

      - name: Run EventosProxParticipantesDeep.py scraper
        timeout-minutes: 30
        env:
          FLOW_EMAIL: "${{ secrets.FLOW_EMAILRQ }}"
          FLOW_PASS: "${{ secrets.FLOW_PASSRQ }}"
        run: |
          echo "=== INICIANDO SCRAPER ==="
          echo "Script: EventosProxParticipantesDeep.py"
          echo "M√≥dulo: all"
          echo "L√≠mite de eventos: ${{ env.MAX_EVENTS_FOR_TESTING }}"
          echo "Timeout m√°ximo: ${{ env.MAX_RUNTIME_MIN }} minutos"
          echo "Directorio salida: ${{ env.OUT_DIR }}"
          echo ""

          python EventosProxParticipantesDeep.py --module all

          SCRAPER_EXIT_CODE=$?
          echo "Scraper finalizado con c√≥digo: $SCRAPER_EXIT_CODE"

          if [ $SCRAPER_EXIT_CODE -eq 0 ]; then
            echo "‚úÖ Scraper ejecutado exitosamente"
          else
            echo "‚ùå Scraper fall√≥ con c√≥digo $SCRAPER_EXIT_CODE"
            # No salir inmediatamente para permitir debugging
          fi

      - name: Verify generated JSON files
        run: |
          echo "=== VERIFICACI√ìN DE ARCHIVOS GENERADOS ==="
          echo "Contenido del directorio ${{ env.OUT_DIR }}:"
          ls -la "${{ env.OUT_DIR }}" || echo "‚ùå No se pudo listar el directorio"

          REQUIRED_FILES=("01events.json" "02info.json")
          ALL_FILES_EXIST=true

          for file in "${REQUIRED_FILES[@]}"; do
            FILE_PATH="${{ env.OUT_DIR }}/$file"
            if [ -f "$FILE_PATH" ]; then
              FILE_SIZE=$(stat -c%s "$FILE_PATH")
              FILE_LINES=$(wc -l < "$FILE_PATH" 2>/dev/null || echo "N/A")
              echo "‚úÖ $file - Tama√±o: ${FILE_SIZE} bytes, L√≠neas: ${FILE_LINES}"
              if [ "$FILE_SIZE" -gt 0 ]; then
                echo "   Primeras 3 l√≠neas:"
                head -3 "$FILE_PATH" | sed 's/^/      /'
              fi
            else
              echo "‚ùå $file - NO ENCONTRADO"
              ALL_FILES_EXIST=false
            fi
            echo ""
          done

          DATED_FILES=$(ls "${{ env.OUT_DIR }}"/0*.json 2>/dev/null | wc -l || echo "0")
          echo "üìÖ Archivos con fecha encontrados: $DATED_FILES"

          if [ "$ALL_FILES_EXIST" = true ]; then
            echo "‚úÖ TODOS los archivos requeridos est√°n presentes"
          else
            echo "‚ùå Faltan algunos archivos requeridos"
          fi

      - name: Analyze 01events.json
        shell: python
        env:
          OUT_DIR: ${{ env.OUT_DIR }}
        run: |
          import os, json
          print("=== AN√ÅLISIS DE CONTENIDO JSON ===")
          out_dir = os.environ.get("OUT_DIR", "./output")
          path = os.path.join(out_dir, "01events.json")
          if not os.path.isfile(path):
              print(f"   01events.json no existe en {path}")
          else:
              print("üìä An√°lisis de 01events.json:")
              try:
                  with open(path, "r", encoding="utf-8") as f:
                      data = json.load(f)
                  print(f"   N√∫mero de eventos: {len(data)}")
                  for i, event in enumerate(data[:3]):
                      print(f"   Evento {i+1}: {event.get('nombre','N/A')}")
                      print(f"     ID: {event.get('id','N/A')}")
                      enlaces = event.get("enlaces", {})
                      print(f"     Enlace participantes: {enlaces.get('participantes','N/A')}")
              except Exception as e:
                  print(f"   Error analizando 01events.json: {e}")

      - name: Analyze 02info.json
        shell: python
        env:
          OUT_DIR: ${{ env.OUT_DIR }}
        run: |
          import os, json
          out_dir = os.environ.get("OUT_DIR", "./output")
          path = os.path.join(out_dir, "02info.json")
          if not os.path.isfile(path):
              print(f"   02info.json no existe en {path}")
          else:
              print("\nüìä An√°lisis de 02info.json:")
              try:
                  with open(path, "r", encoding="utf-8") as f:
                      data = json.load(f)
                  print(f"   N√∫mero de eventos procesados: {len(data)}")
                  events_with_participants = [e for e in data if e.get("numero_participantes", 0) > 0]
                  total_participants = sum(e.get("numero_participantes", 0) for e in data)
                  print(f"   Eventos con participantes: {len(events_with_participants)}")
                  print(f"   Total participantes: {total_participants}")
                  if events_with_participants:
                      print("   Eventos con m√°s participantes:")
                      for event in sorted(events_with_participants, key=lambda x: x.get("numero_participantes", 0), reverse=True)[:3]:
                          print(f"     - {event.get('nombre','N/A')}: {event.get('numero_participantes')} participantes")
              except Exception as e:
                  print(f"   Error analizando 02info.json: {e}")

PY

          # --- Script para 02info.json ---
          cat > /tmp/analyze_info.py << 'PY'
import json, os, sys
out_dir = os.environ.get("OUT_DIR", "./output")
path = os.path.join(out_dir, "02info.json")
try:
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    print(f"   N√∫mero de eventos procesados: {len(data)}")
    events_with_participants = [e for e in data if e.get("numero_participantes", 0) > 0]
    total_participants = sum(e.get("numero_participantes", 0) for e in data)
    print(f"   Eventos con participantes: {len(events_with_participants)}")
    print(f"   Total participantes: {total_participants}")
    if events_with_participants:
        print("   Eventos con m√°s participantes:")
        for event in sorted(events_with_participants, key=lambda x: x.get("numero_participantes", 0), reverse=True)[:3]:
            print(f"     - {event.get('nombre', 'N/A')}: {event.get('numero_participantes')} participantes")
except FileNotFoundError:
    print(f"   02info.json no existe en {path}")
except Exception as e:
    print(f"   Error analizando 02info.json: {e}")
PY

          # Ejecutar an√°lisis si existen los archivos
          if [ -f "${{ env.OUT_DIR }}/01events.json" ]; then
            echo "üìä An√°lisis de 01events.json:"
            python /tmp/analyze_events.py
          fi

          if [ -f "${{ env.OUT_DIR }}/02info.json" ]; then
            echo ""
            echo "üìä An√°lisis de 02info.json:"
            python /tmp/analyze_info.py
          fi


      - name: Compress JSON files for backup
        run: |
          echo "=== COMPRIMIENDO ARCHIVOS PARA BACKUP ==="
          for json_file in "${{ env.OUT_DIR }}"/*.json; do
            if [ -f "$json_file" ]; then
              gzip -9 -k "$json_file"
              original_size=$(stat -c%s "$json_file")
              compressed_size=$(stat -c%s "${json_file}.gz")
              compression_ratio=$((compressed_size * 100 / original_size))
              echo "‚úÖ $(basename "$json_file"): ${original_size} bytes ‚Üí ${compressed_size} bytes (${compression_ratio}%)"
            fi
          done

          echo ""
          echo "Archivos comprimidos:"
          ls -la "${{ env.OUT_DIR }}"/*.gz 2>/dev/null || echo "No hay archivos comprimidos"

      - name: Create backup directory
        run: |
          mkdir -p ./backup
          echo "Directorio backup creado"

      - name: Backup uncompressed files
        run: |
          echo "=== CREANDO BACKUP DE ARCHIVOS SIN COMPRIMIR ==="
          cp -r "${{ env.OUT_DIR }}" ./backup/ || echo "No se pudo crear backup"
          echo "Backup creado en ./backup/output/"
          ls -la ./backup/ || echo "No hay backup"

      - name: Upload JSON files as artifact
        uses: actions/upload-artifact@v4
        with:
          name: scraper-json-output
          path: |
            ${{ env.OUT_DIR }}/*.json
            ${{ env.OUT_DIR }}/*.gz
          retention-days: 7
          if-no-files-found: warn

      - name: Upload backup as artifact
        uses: actions/upload-artifact@v4
        with:
          name: scraper-backup
          path: |
            ./backup/
          retention-days: 3
          if-no-files-found: warn

      - name: Upload logs and debugging info
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: |
            ./*.log
            /tmp/*.png
            /home/runner/.local/share/chromium/
          retention-days: 1
          if-no-files-found: ignore

      - name: Final summary
        run: |
          echo "=== RESUMEN FINAL DEL WORKFLOW ==="
          echo "üéØ Configuraci√≥n utilizada:"
          echo "   - L√≠mite de eventos: ${{ env.MAX_EVENTS_FOR_TESTING }}"
          echo "   - M√°ximo tiempo ejecuci√≥n: ${{ env.MAX_RUNTIME_MIN }} minutos"
          echo "   - Directorio salida: ${{ env.OUT_DIR }}"

          echo ""
          echo "üìÅ Archivos generados:"
          if [ -d "${{ env.OUT_DIR }}" ]; then
            for file in "${{ env.OUT_DIR }}"/*; do
              if [ -f "$file" ]; then
                size=$(stat -c%s "$file" 2>/dev/null || echo "N/A")
                echo "   - $(basename "$file"): $size bytes"
              fi
            done
          else
            echo "   ‚ùå No se generaron archivos"
          fi

          echo ""
          echo "‚è∞ Hora de finalizaci√≥n: $(date)"
          echo "‚úÖ Workflow completado"

      - name: Debug on failure
        if: failure()
        run: |
          echo "=== DEBUGGING POR FALLO ==="
          echo "‚ùå El workflow ha fallado"

          echo ""
          echo "üìã Informaci√≥n del sistema:"
          echo "Ubuntu version: $(lsb_release -d || echo 'N/A')"
          echo "Python version: $(python --version || echo 'N/A')"
          echo "Chrome version: $(google-chrome --version || echo 'N/A')"
          echo "ChromeDriver version: $(chromedriver --version || echo 'N/A')"

          echo ""
          echo "üìÅ Estructura de directorios:"
          echo "Directorio actual:"
          ls -la
          echo ""
          echo "Directorio output:"
          ls -la "${{ env.OUT_DIR }}" 2>/dev/null || echo "No existe"
          echo ""
          echo "Directorio backup:"
          ls -la "./backup" 2>/dev/null || echo "No existe"

          echo ""
          echo "üîç Procesos Chrome activos:"
          ps aux | grep -i chrome || echo "No hay procesos Chrome"

          echo ""
          echo "üìä √öltimos logs del sistema:"
          dmesg | tail -20 || echo "No se pueden leer logs del sistema"

  # Job opcional para despliegue (solo en ejecuciones manuales)
  deploy-to-ftp:
    if: ${{ github.event_name == 'workflow_dispatch' }}
    needs: run-scrape
    runs-on: ubuntu-24.04
    timeout-minutes: 10

    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          name: scraper-json-output
          path: ./artifacts

      - name: Check downloaded files
        run: |
          echo "=== ARCHIVOS DESCARGADOS ==="
          find ./artifacts -type f -name "*.json" -o -name "*.gz" | sort
          echo ""
          echo "Contenido:"
          ls -la ./artifacts/

      - name: Upload to FTP (conditional)
        if: ${{ env.FTP_SERVER != '' && env.FTP_USERNAME != '' && env.FTP_PASSWORD != '' }}
        env:
          FTP_SERVER: ${{ secrets.FTP_SERVER }}
          FTP_USERNAME: ${{ secrets.FTP_USERNAME }}
          FTP_PASSWORD: ${{ secrets.FTP_PASSWORD }}
          FTP_REMOTE_DIR: ${{ secrets.FTP_REMOTE_DIR }}
        run: |
          echo "=== SUBIENDO A FTP (MODO PRUEBAS) ==="

          if [ -z "$FTP_SERVER" ] || [ -z "$FTP_USERNAME" ] || [ -z "$FTP_PASSWORD" ]; then
            echo "‚ö†Ô∏è  Credenciales FTP no configuradas - saltando subida"
            exit 0
          fi

          REMOTE_DIR="${FTP_REMOTE_DIR}/Competiciones/EventosProx/Flow/data/TEST"
          BASE_URL="ftp://${FTP_SERVER}${REMOTE_DIR}"

          echo "Subiendo a: ${BASE_URL}/"

          # Crear directorio remoto (si no existe)
          curl --fail --silent \
               --ssl-reqd \
               --user "${FTP_USERNAME}:${FTP_PASSWORD}" \
               -Q "MKD ${REMOTE_DIR}" \
               "ftp://${FTP_SERVER}/" || echo "Directorio posiblemente ya existe"

          # Subir archivos JSON
          for json_file in ./artifacts/*.json; do
            if [ -f "$json_file" ]; then
              filename=$(basename "$json_file")
              echo "üì§ Subiendo: $filename"

              curl --fail --silent \
                   --ssl-reqd \
                   --user "${FTP_USERNAME}:${FTP_PASSWORD}" \
                   --upload-file "$json_file" \
                   "${BASE_URL}/$filename" && \
              echo "‚úÖ $filename subido exitosamente" || \
              echo "‚ùå Error subiendo $filename"
            fi
          done

          echo "‚úÖ Subida FTP completada"
